{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def mnist():\n",
    "    \"\"\"Return train and test dataloaders for MNIST.\"\"\"\n",
    "    # exchange with the corrupted mnist dataset\n",
    "    path = \"/Users/sorenbendtsen/Documents/GitHub/dtu_mlops/data/corruptmnist/\"\n",
    "    train_images, train_labels = [], []\n",
    "    for i in range(1, 6):\n",
    "        train_images.append(torch.load(path + \"train_images_\" + str(i) + \".pt\"))\n",
    "        train_labels.append(torch.load(path + \"train_target_\" + str(i) + \".pt\"))\n",
    "\n",
    "    test_images = torch.load(path + \"test_images.pt\")\n",
    "    test_labels = torch.load(path + \"test_target.pt\")\n",
    "    \n",
    "    # stack the tensors\n",
    "    train_images = torch.cat(train_images)\n",
    "    train_labels = torch.cat(train_labels)\n",
    "\n",
    "    # normalize the images with mean 0 and std 1\n",
    "    mean = train_images.mean()\n",
    "    std = train_images.std()\n",
    "    for i in range(len(train_images)):\n",
    "        train_images[i] = (train_images[i] - mean) / std\n",
    "    \n",
    "    # convert to torch tensors\n",
    "    train = torch.utils.data.TensorDataset(train_images, train_labels)\n",
    "    test = torch.utils.data.TensorDataset(test_images, test_labels)\n",
    "\n",
    "    # create dataloaders\n",
    "    train = torch.utils.data.DataLoader(train, batch_size=32, shuffle=True)\n",
    "    test = torch.utils.data.DataLoader(test, batch_size=32, shuffle=False)\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28, 28])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.4260, -0.3881, -0.3754, -0.3248, -0.2995, -0.1604, -0.1350, -0.0971,\n",
       "        -0.0338,  0.0294,  0.0547,  0.0674,  0.0800,  0.0927,  0.1433,  0.3078,\n",
       "         0.3457,  0.3963,  0.4849,  0.5102,  0.5355,  0.5608,  0.6114,  0.6494,\n",
       "         0.6620,  0.6873,  0.7253,  0.7759,  0.8012,  0.8644,  0.9277,  0.9530,\n",
       "         0.9657,  0.9910,  1.1807,  1.2693,  1.3326,  1.3579,  1.5476,  1.5856,\n",
       "         1.6236,  1.6489,  1.8133,  1.8260,  1.8513,  1.9652,  1.9905,  2.0284,\n",
       "         2.0664,  2.1549,  2.2182,  2.2435,  2.3068,  2.3194,  2.3574,  2.3827,\n",
       "         2.4206,  2.4333,  2.4459,  2.4712,  2.5092,  2.5345,  2.5851,  2.6610,\n",
       "         2.6990,  2.7116,  2.7243,  2.7369,  2.7749,  2.8002])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = next(iter(train))[0][2]\n",
    "print(img.shape)\n",
    "img.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/sorenbendtsen/Documents/GitHub/dtu_mlops/data/corruptmnist/\"\n",
    "train_images, train_labels = [], []\n",
    "for i in range(1, 6):\n",
    "    train_images.append(torch.load(path + \"train_images_\" + str(i) + \".pt\"))\n",
    "    train_labels.append(torch.load(path + \"train_target_\" + str(i) + \".pt\"))\n",
    "\n",
    "test_images = torch.load(path + \"test_images.pt\")\n",
    "test_labels = torch.load(path + \"test_target.pt\")\n",
    "    \n",
    "# stack the tensors\n",
    "train_images = torch.cat(train_images)\n",
    "train_labels = torch.cat(train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4260, -0.2869, -0.1730, -0.1604, -0.0465, -0.0338,  0.0800,  0.0927,\n",
       "         0.2066,  0.2192,  0.3584,  0.6114,  0.7379,  0.8644,  0.9910,  1.0036,\n",
       "         1.1301,  1.3705,  1.4844,  1.4970,  1.7374,  1.7501,  1.7627,  1.8892,\n",
       "         2.0031,  2.1423,  2.2561,  2.2688,  2.2815,  2.3953,  2.4080,  2.5092,\n",
       "         2.5218,  2.6484,  2.6610,  2.7622,  2.7749,  2.7875,  2.8002])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images[1].unique()\n",
    "mean = train_images.mean()\n",
    "std = train_images.std()\n",
    "train_images[1] = (train_images[1] - mean) / std\n",
    "# normalize the images so that they have mean 0 and std 1\n",
    "train_images[1].unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
